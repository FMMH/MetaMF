{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from MetaMF import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize random seeds and select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1) # set random seed for cpu\n",
    "torch.cuda.manual_seed(1) # set random seed for current gpu\n",
    "torch.cuda.manual_seed_all(1) # set random seed for all gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    use_cuda = False\n",
    "print(\"CUDA available? \" + str(use_cuda))\n",
    "if use_cuda:\n",
    "    print(\"Current device: %d\" % torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    trainset = pd.read_csv(path + \".train.rating\", sep = ',', header=None).to_records(index=False).tolist()\n",
    "    valset = pd.read_csv(path + \".valid.rating\", sep = ',', header=None).to_records(index=False).tolist()\n",
    "    testset = pd.read_csv(path + \".test.rating\", sep = ',', header=None).to_records(index=False).tolist()\n",
    "    \n",
    "    return trainset, valset, testset\n",
    "\n",
    "def read_usergroups(path):\n",
    "    low_users = pd.read_csv(path + \"_low.userlist\", header=None, squeeze=True).values.tolist()\n",
    "    med_users = pd.read_csv(path + \"_med.userlist\", header=None, squeeze=True).values.tolist()\n",
    "    high_users = pd.read_csv(path + \"_high.userlist\", header=None, squeeze=True).values.tolist()\n",
    "    \n",
    "    return low_users, med_users, high_users\n",
    "\n",
    "def read_useranditemlist(path):\n",
    "    userlist = pd.read_csv(path + \".userlist\", header=None, squeeze=True).values.tolist()\n",
    "    itemlist = pd.read_csv(path + \".itemlist\", header=None, squeeze=True).values.tolist()\n",
    "    \n",
    "    return userlist, itemlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchtoinput(batch, use_cuda):\n",
    "    users = []\n",
    "    items = []\n",
    "    ratings = []\n",
    "    timestamps = []\n",
    "    for example in batch:\n",
    "        users.append(example[0])\n",
    "        items.append(example[1])\n",
    "        ratings.append(example[2])\n",
    "        timestamps.append(example[3])\n",
    "    users = torch.tensor(users, dtype=torch.int64)\n",
    "    items = torch.tensor(items, dtype=torch.int64)\n",
    "    ratings = torch.tensor(ratings, dtype=torch.float32)\n",
    "    timestamps = torch.tensor(timestamps, dtype=torch.int64)\n",
    "    if use_cuda:\n",
    "        users = users.cuda()\n",
    "        items = items.cuda()\n",
    "        ratings = ratings.cuda()\n",
    "        timestamps = timestamps.cuda()\n",
    "    return users, items, ratings, timestamps\n",
    "\n",
    "def getbatches(traindata, batch_size, use_cuda, shuffle):\n",
    "    dataset = traindata.copy()\n",
    "    if shuffle:\n",
    "        random.shuffle(dataset)\n",
    "    for batch_i in range(0,int(np.ceil(len(dataset)/batch_size))):\n",
    "        start_i = batch_i*batch_size\n",
    "        batch = dataset[start_i:start_i+batch_size]\n",
    "        yield batchtoinput(batch, use_cuda)\n",
    "        \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "def get_eval(ratlist, predlist):\n",
    "    mae = np.mean(np.abs(ratlist-predlist))\n",
    "    mse = np.mean(np.square(ratlist-predlist))       \n",
    "    return  mae, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Functions\n",
    "## Random Sampling Procedure\n",
    "Here, we implement a sampling procedure to simulate a privacy budget $\\beta$. In detail, we randomly select a fraction of $\\beta$ of each user's rating data to be shared with the model. Thus, a user holds back a fraction of $1-\\beta$ of her data and provides only a fraction of $\\beta$ of her data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_procedure(dataset, beta):\n",
    "    dataframe = pd.DataFrame(dataset, columns=[\"user_id\", \"item_id\", \"rating\"])\n",
    "    n_samples = np.ceil(dataframe.groupby(\"user_id\").size() * (beta)).astype(int)\n",
    "    new_dataset = []\n",
    "    for uid, group in dataframe.groupby(\"user_id\"):\n",
    "        new_dataset.extend(group.sample(n=n_samples.loc[uid]).to_records(index=False).tolist())\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Procedure by time\n",
    "This sampling procedure takes the timestamp of a rating into account when creating training subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_procedure(dataset, beta):\n",
    "    dataframe = pd.DataFrame(dataset, columns=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "    n_samples = dataframe.sort_values(by=['timestamp', 'user_id'],ascending=False).groupby('user_id').size().reset_index() \n",
    "    n_samples.columns = ['user_id','count']\n",
    "    n_samples.loc[:,'count'] = n_samples[['count']] * (beta)\n",
    "    n_samples.loc[:,'count'] = n_samples['count'].astype(int)\n",
    "    users = dataframe['user_id'].unique().tolist()\n",
    "    new_dataset = []\n",
    "    for user in users:\n",
    "        userID = n_samples.loc[n_samples['user_id']==user,'count'].values[0]\n",
    "        if userID >= 0:\n",
    "            temp_df = dataframe[dataframe['user_id']==user]\n",
    "            temp_df = temp_df.iloc[:userID,:]\n",
    "            new_dataset.extend(temp_df.values)\n",
    "    new_dataset = pd.DataFrame(new_dataset, columns=dataframe.columns)\n",
    "    new_dataset.to_csv(r'C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown09.csv', index = False)\n",
    "    return new_dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in case the temporal sampling procedure was used, the data that is to be augmented is identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0        0        0     5.0  978300760\n",
      "1        0        1     3.0  978302109\n",
      "2        0        2     3.0  978301968\n",
      "3        0        3     4.0  978300275\n",
      "4        0        5     3.0  978302268\n",
      "   user_id  item_id  rating    timestamp\n",
      "0      0.0      0.0     5.0  978300760.0\n",
      "1      0.0      1.0     3.0  978302109.0\n",
      "2      0.0      2.0     3.0  978301968.0\n",
      "3      0.0      3.0     4.0  978300275.0\n",
      "4      0.0      5.0     3.0  978302268.0\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m.train.rating', names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "RecData = sampling_procedure(train, beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  item_id  rating  timestamp\n",
      "0             0        0       5  978300760\n",
      "1             0        1       3  978302109\n",
      "2             0        2       3  978301968\n",
      "3             0        3       4  978300275\n",
      "4             0        5       3  978302268\n",
      "...         ...      ...     ...        ...\n",
      "810278     6039      772       1  956716541\n",
      "810279     6039     1106       5  956704887\n",
      "810280     6039      365       5  956704746\n",
      "810281     6039      152       4  956715648\n",
      "810282     6039       26       4  956715569\n",
      "\n",
      "[810283 rows x 4 columns]\n",
      "        user_id  item_id  rating  timestamp\n",
      "0             0        0       5  978300760\n",
      "1             0        1       3  978302109\n",
      "2             0        2       3  978301968\n",
      "3             0        3       4  978300275\n",
      "4             0        5       3  978302268\n",
      "...         ...      ...     ...        ...\n",
      "726525     6039      604       1  956716407\n",
      "726526     6039     1049       2  956715942\n",
      "726527     6039     2425       3  956716157\n",
      "726528     6039      635       2  956716157\n",
      "726529     6039     1962       2  956715865\n",
      "\n",
      "[726530 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Some datatypes have to be corrected\n",
    "\n",
    "ff = train.select_dtypes(include = ['float64'])\n",
    "for col in ff.columns.values:\n",
    "    train[col] = train[col].astype('int64')\n",
    "print(train)\n",
    "\n",
    "cc = RecData.select_dtypes(include = ['float64'])\n",
    "for col in cc.columns.values:\n",
    "    RecData[col] = RecData[col].astype('int64')\n",
    "print(RecData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the hidden data as all train data minus the rows used in RecData\n",
    "\n",
    "cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df2 = train.set_index(cols)\n",
    "RecData2 = RecData.set_index(cols)\n",
    "hidden_data = train[~df2.index.isin(RecData2.index)]\n",
    "\n",
    "hidden_data.to_csv('C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_hidden10p.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown09.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-80f753f9f6c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshown\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown09.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mreshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown09.csv'"
     ]
    }
   ],
   "source": [
    "shown = pd.read_csv('C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown09.csv')\n",
    "\n",
    "reshape = shown.select_dtypes(include = ['float64'])\n",
    "for col in reshape.columns.values:\n",
    "    reshape[col] = reshape[col].astype('int64')\n",
    "\n",
    "reshape.to_csv('C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown09p.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then merge and sort both the augmented and original dataset back into one full training set\n",
    "\n",
    "dataS = pd.read_csv(\"C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/ml-1m_shown60p.csv\")\n",
    "df_shown = pd.DataFrame(dataS)\n",
    "del df_shown[\"timestamp\"]\n",
    "\n",
    "dataH = pd.read_csv(\"C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/augmented/sd40p.csv\")\n",
    "df_hidden = pd.DataFrame(dataH)\n",
    "\n",
    "total = [df_shown, df_hidden]\n",
    "final = pd.concat(total)\n",
    "new_train_set = final.sort_values(by=['user_id', 'item_id'], ascending=True)\n",
    "new_train_set.to_csv(\"C:/Users/fleur/Thesis B3/RQ0/RobustnessOfMetaMF-master/RobustnessOfMetaMF-master/ThesisData/ml-1m/ml-1m_time/augmented/ml-1m06p.train.rating\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "This method trains and tests MetaMF and NoMetaMF under ten different privacy budgets (i.e., $\\beta \\in \\{1.0, 0.9, \\dots, 0.1\\}$). For NoMetaMF, meta learning the parameters of the rating prediction model can be disabled. Furthermore, we evaluate the model's accuracy in terms of the mean squared error and the mean absolute error on both, all users in the dataset and on our three user groups (i.e., $Low, Med, High$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(path, traindata, valdata, testdata, userlist, itemlist, low, med, high, hyperparameters, betas=None, disable_meta_learning=False, save=False):\n",
    "    # default choice of privacy budget beta\n",
    "    if betas is None:\n",
    "        betas = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "        \n",
    "    if os.path.exists(path + \"/results.csv\"):\n",
    "        results_df = pd.read_csv(path + \"/results.csv\")\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "        \n",
    "    for beta in betas:\n",
    "        results_dict = {\"beta\": beta}\n",
    "        model_name = \"beta_\" + str(int(beta*100)) + \"p\"  \n",
    "        print(\"==========================\")\n",
    "        print(model_name)\n",
    "        print(\"==========================\")\n",
    "        starttime = dt.now()\n",
    "        \n",
    "        # sample a fraction of beta of each user's data to simulate privacy budget beta\n",
    "        R_train_beta = sampling_procedure(traindata, beta)\n",
    "        \n",
    "        train_loss, validation_loss = [], []\n",
    "        net = MetaMF(len(userlist), len(itemlist))\n",
    "        \n",
    "        # disable meta learning for NoMetaMF\n",
    "        if disable_meta_learning:\n",
    "            net.disable_meta_learning()\n",
    "        \n",
    "        # initialize parameters of neural network\n",
    "        net.apply(weights_init)\n",
    "        if use_cuda:\n",
    "            net.cuda()\n",
    "        \n",
    "        # model training\n",
    "        optimizer = optim.Adam(net.parameters(), lr=hyperparameters[\"lr\"], weight_decay=hyperparameters[\"lambda\"])\n",
    "        batch_size = hyperparameters[\"batch_size\"]\n",
    "        n_epochs = hyperparameters[\"n_epochs\"]\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            net.train()\n",
    "            error = 0\n",
    "            num = 0\n",
    "            for k, (users, items, ratings, timestamps) in enumerate(getbatches(R_train_beta, batch_size, use_cuda, True)):\n",
    "                optimizer.zero_grad()\n",
    "                pred = net(users, items)\n",
    "\n",
    "                loss = net.loss(pred, ratings)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "                optimizer.step()\n",
    "                error += loss.detach().cpu().numpy()*len(users)\n",
    "                num += len(users)\n",
    "            train_loss.append(error/num)\n",
    "            \n",
    "            # evaluate training error\n",
    "            net.eval()\n",
    "            groundtruth, estimation = [], []\n",
    "            for users, items, ratings, timestamps in getbatches(valdata, batch_size, use_cuda, False):\n",
    "                predictions = net(users, items)\n",
    "                estimation.extend(predictions.tolist())\n",
    "                groundtruth.extend(ratings.tolist())\n",
    "            mae, mse = get_eval(np.array(groundtruth), np.array(estimation))\n",
    "            validation_loss.append(mse)\n",
    "            \n",
    "            print('Epoch {}/{} - Training Loss: {:.3f}, Validation Loss: {:.3f}, Time Elapsed: {}'.format(epoch+1, n_epochs, error/num, mse, dt.now()-starttime))\n",
    "            \n",
    "            if epoch+1 == n_epochs:\n",
    "                if save:\n",
    "                    torch.save(net, path + \"/\" + model_name + '.model')\n",
    "                    print(\"Saved Model to \" + path)\n",
    "                \n",
    "                results_dict[\"train_mse_all\"] = error / num\n",
    "                results_dict[\"val_mse_all\"] = mse\n",
    "        \n",
    "        # plot training and validation error to observe convergence\n",
    "        net.eval()\n",
    "        plt.figure()\n",
    "        plt.plot(range(n_epochs), train_loss, label=\"Train\")\n",
    "        plt.plot(range(n_epochs), validation_loss, label=\"Val\")\n",
    "        plt.legend()\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # evaluate test error on both, all users in the dataset and on our three user groups\n",
    "        groundtruth, estimation = [], []\n",
    "        group_groundtruth = defaultdict(list)\n",
    "        group_estimation = defaultdict(list)\n",
    "        for users, items, ratings, timestamps in getbatches(testdata, batch_size, use_cuda, False):\n",
    "            predictions = net(users, items)\n",
    "            estimation.extend(predictions.tolist())\n",
    "            groundtruth.extend(ratings.tolist())\n",
    "            \n",
    "            for uid, iid, r, p in zip(users.cpu().numpy(), items.cpu().numpy(), ratings.cpu().numpy(), predictions.detach().cpu().numpy()):\n",
    "                if uid in low:\n",
    "                    group_groundtruth[\"low\"].append(r)\n",
    "                    group_estimation[\"low\"].append(p)\n",
    "                elif uid in med:\n",
    "                    group_groundtruth[\"med\"].append(r)\n",
    "                    group_estimation[\"med\"].append(p)\n",
    "                elif uid in high:\n",
    "                    group_groundtruth[\"high\"].append(r)\n",
    "                    group_estimation[\"high\"].append(p)\n",
    "        \n",
    "        test_mae, test_mse = get_eval(np.array(groundtruth), np.array(estimation))\n",
    "        low_mae, low_mse = get_eval(np.array(group_groundtruth[\"low\"]), np.array(group_estimation[\"low\"]))\n",
    "        med_mae, med_mse = get_eval(np.array(group_groundtruth[\"med\"]), np.array(group_estimation[\"med\"]))\n",
    "        high_mae, high_mse = get_eval(np.array(group_groundtruth[\"high\"]), np.array(group_estimation[\"high\"]))\n",
    "        \n",
    "        results_dict[\"test_mse_all\"] = test_mse\n",
    "        results_dict[\"test_mae_all\"] = test_mae\n",
    "        results_dict[\"test_mse_low\"] = low_mse\n",
    "        results_dict[\"test_mae_low\"] = low_mae\n",
    "        results_dict[\"test_mse_med\"] = med_mse\n",
    "        results_dict[\"test_mae_med\"] = med_mae\n",
    "        results_dict[\"test_mse_high\"] = high_mse\n",
    "        results_dict[\"test_mae_high\"] = high_mae\n",
    "        \n",
    "        print(results_dict)\n",
    "        if save:\n",
    "            plt.savefig(path + \"/\" + model_name + \".png\", dpi=300)\n",
    "            results_df = results_df.append(pd.DataFrame([results_dict]))\n",
    "            results_df.to_csv(path + \"/results.csv\", index=False)\n",
    "            print(\"Saved Results to \" + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens 1M run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "beta_10p\n",
      "==========================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-71f2578fe6af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_usergroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/User Groups/time/ml1m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m run(\"experiments/timemeta/ml1m\", train, val, test, users, items, low, med, high, save=True, disable_meta_learning=False, betas=[0.1],\n\u001b[0m\u001b[0;32m      6\u001b[0m     hyperparameters={\"lr\": 0.0001, \"lambda\": 0.001, \"batch_size\": 64, \"n_epochs\": 1})\n",
      "\u001b[1;32m<ipython-input-7-4484b7cfb744>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(path, traindata, valdata, testdata, userlist, itemlist, low, med, high, hyperparameters, betas, disable_meta_learning, save)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# sample a fraction of beta of each user's data to simulate privacy budget beta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mR_train_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampling_procedure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-45eb6faf4096>\u001b[0m in \u001b[0;36msampling_procedure\u001b[1;34m(dataset, beta)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0muserID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muserID\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0muserID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mnew_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0muse_numexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mb_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         result = ne.evaluate(\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[1;34mf\"a_value {op_str} b_value\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mlocal_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"a_value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b_value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb_value\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numexpr\\necompiler.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[0m_numexpr_last\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompiled_ex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mevaluate_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcompiled_ex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train, val, test = read_dataset(\"ThesisData/ml-1m/ml-1m/ml-1m\")\n",
    "users, items = read_useranditemlist(\"ThesisData/ml-1m/ml-1m/ml-1m\")\n",
    "\n",
    "low, med, high = read_usergroups(\"data/User Groups/time/ml1m\")\n",
    "run(\"experiments/timemeta/ml1m\", train, val, test, users, items, low, med, high, save=True, disable_meta_learning=False, betas=[0.1],\n",
    "    hyperparameters={\"lr\": 0.0001, \"lambda\": 0.001, \"batch_size\": 64, \"n_epochs\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens Latest Small run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "beta_100p\n",
      "==========================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid index in scatter at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:151",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8c61ff4e3ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_usergroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/User Groups/mllatestsmall\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m run(\"experiments/meta/ml-latest-small\", train, val, test, users, items, low, med, high, save=True, disable_meta_learning=False, \n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     hyperparameters={\"lr\": 0.0001, \"lambda\": 0.001, \"batch_size\": 64, \"n_epochs\": 100})\n",
      "\u001b[1;32m<ipython-input-7-8f155e8d4ce9>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(path, traindata, valdata, testdata, userlist, itemlist, low, med, high, hyperparameters, betas, disable_meta_learning, save)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetbatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR_train_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Thesis B3\\RQ0\\RobustnessOfMetaMF-master\\RobustnessOfMetaMF-master\\MetaMF.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, user_id, item_id)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mitem_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mitem_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mitem_one_hot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mitem_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mitem_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Invalid index in scatter at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:151"
     ]
    }
   ],
   "source": [
    "train, val, test = read_dataset(\"ThesisData/ml-latest-small/ml-latest-small/ml-latest-small\")\n",
    "users, items = read_useranditemlist(\"ThesisData/ml-latest-small/ml-latest-small/ml-latest-small\")\n",
    "\n",
    "low, med, high = read_usergroups(\"data/User Groups/mllatestsmall\")\n",
    "run(\"experiments/meta/ml-latest-small\", train, val, test, users, items, low, med, high, save=True, disable_meta_learning=False, \n",
    "    hyperparameters={\"lr\": 0.0001, \"lambda\": 0.001, \"batch_size\": 64, \"n_epochs\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens 100K run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "beta_100p\n",
      "==========================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No axis named timestamp for object type DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[1;34m(cls, axis)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_TO_AXIS_NUMBER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timestamp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-885fbcc2d526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_usergroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/User Groups/time/ml100k\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m run(\"experiments/timemeta/ml100k\", train, val, test, users, items, low, med, high, save=True, disable_meta_learning=False, \n\u001b[0m\u001b[0;32m      6\u001b[0m     hyperparameters={\"lr\": 0.0001, \"lambda\": 0.001, \"batch_size\": 64, \"n_epochs\": 1})\n",
      "\u001b[1;32m<ipython-input-9-744a5271da99>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(path, traindata, valdata, testdata, userlist, itemlist, low, med, high, hyperparameters, betas, disable_meta_learning, save)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# sample a fraction of beta of each user's data to simulate privacy budget beta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mR_train_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampling_procedure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-effbe0af6db4>\u001b[0m in \u001b[0;36msampling_procedure\u001b[1;34m(dataset, beta)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msampling_procedure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"user_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"item_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rating\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnew_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   6507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mby\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6508\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6509\u001b[1;33m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6511\u001b[0m         return DataFrameGroupBy(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[1;34m(cls, axis)\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_TO_AXIS_NUMBER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No axis named {axis} for object type {cls.__name__}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No axis named timestamp for object type DataFrame"
     ]
    }
   ],
   "source": [
    "train, val, test = read_dataset(\"ThesisData/ml-100k/ml-100k/ml-100k\")\n",
    "users, items = read_useranditemlist(\"ThesisData/ml-100k/ml-100k/ml-100k\")\n",
    "\n",
    "low, med, high = read_usergroups(\"data/User Groups/ml100k\")\n",
    "run(\"experiments/timemeta/ml100k\", train, val, test, users, items, low, med, high, save=True, disable_meta_learning=False, \n",
    "    hyperparameters={\"lr\": 0.0001, \"lambda\": 0.001, \"batch_size\": 64, \"n_epochs\": 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
